
  0%|                                                                                                                                                                                 | 0/100 [00:00<?, ?it/s]
image embeddings shape:  torch.Size([4, 1000])
image embeddings shape:  torch.Size([4, 1000])
image embeddings shape:  torch.Size([4, 1000])
image embeddings shape:  torch.Size([4, 1000])
image embeddings shape:  torch.Size([4, 1000])
image embeddings shape:  torch.Size([4, 1000])
image embeddings shape:  torch.Size([4, 1000])

  1%|█▋                                                                                                                                                                    | 1/100 [01:45<2:54:20, 105.66s/it]
image embeddings shape:  torch.Size([4, 1000])
image embeddings shape:  torch.Size([4, 1000])
image embeddings shape:  torch.Size([4, 1000])
image embeddings shape:  torch.Size([4, 1000])
image embeddings shape:  torch.Size([4, 1000])
image embeddings shape:  torch.Size([4, 1000])
image embeddings shape:  torch.Size([4, 1000])

  2%|███▎                                                                                                                                                                  | 2/100 [03:23<2:44:49, 100.91s/it]
image embeddings shape:  torch.Size([4, 1000])
image embeddings shape:  torch.Size([4, 1000])
image embeddings shape:  torch.Size([3, 1000])
  2%|███▎                                                                                                                                                                  | 2/100 [03:23<2:44:49, 100.91s/it]Traceback (most recent call last):
  File "/Users/rijudey/Documents/GitHub/AppliedComputerVision/classifierModels/train.py", line 200, in <module>
    train(**owlresnet_train_dict)
  File "/Users/rijudey/Documents/GitHub/AppliedComputerVision/classifierModels/train.py", line 95, in train
    train_results = trainer.train()
  File "/Users/rijudey/Documents/GitHub/AppliedComputerVision/diningHallEnv/lib/python3.9/site-packages/transformers/trainer.py", line 1780, in train
    return inner_training_loop(
  File "/Users/rijudey/Documents/GitHub/AppliedComputerVision/diningHallEnv/lib/python3.9/site-packages/transformers/trainer.py", line 2213, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
  File "/Users/rijudey/Documents/GitHub/AppliedComputerVision/diningHallEnv/lib/python3.9/site-packages/transformers/trainer.py", line 2577, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/Users/rijudey/Documents/GitHub/AppliedComputerVision/diningHallEnv/lib/python3.9/site-packages/transformers/trainer.py", line 3365, in evaluate
    output = eval_loop(
  File "/Users/rijudey/Documents/GitHub/AppliedComputerVision/diningHallEnv/lib/python3.9/site-packages/transformers/trainer.py", line 3554, in evaluation_loop
    loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
  File "/Users/rijudey/Documents/GitHub/AppliedComputerVision/diningHallEnv/lib/python3.9/site-packages/transformers/trainer.py", line 3771, in prediction_step
    loss, outputs = self.compute_loss(model, inputs, return_outputs=True)
  File "/Users/rijudey/Documents/GitHub/AppliedComputerVision/diningHallEnv/lib/python3.9/site-packages/transformers/trainer.py", line 3059, in compute_loss
    outputs = model(**inputs)
  File "/Users/rijudey/Documents/GitHub/AppliedComputerVision/diningHallEnv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rijudey/Documents/GitHub/AppliedComputerVision/diningHallEnv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rijudey/Documents/GitHub/AppliedComputerVision/classifierModels/concatModels/OwLResNet.py", line 77, in forward
    vit_output = self.vit(**inputs)
  File "/Users/rijudey/Documents/GitHub/AppliedComputerVision/diningHallEnv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rijudey/Documents/GitHub/AppliedComputerVision/diningHallEnv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rijudey/Documents/GitHub/AppliedComputerVision/diningHallEnv/lib/python3.9/site-packages/transformers/models/owlv2/modeling_owlv2.py", line 1006, in forward
    return self.vision_model(
  File "/Users/rijudey/Documents/GitHub/AppliedComputerVision/diningHallEnv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rijudey/Documents/GitHub/AppliedComputerVision/diningHallEnv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rijudey/Documents/GitHub/AppliedComputerVision/diningHallEnv/lib/python3.9/site-packages/transformers/models/owlv2/modeling_owlv2.py", line 940, in forward
    encoder_outputs = self.encoder(
  File "/Users/rijudey/Documents/GitHub/AppliedComputerVision/diningHallEnv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rijudey/Documents/GitHub/AppliedComputerVision/diningHallEnv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rijudey/Documents/GitHub/AppliedComputerVision/diningHallEnv/lib/python3.9/site-packages/transformers/models/owlv2/modeling_owlv2.py", line 754, in forward
    layer_outputs = encoder_layer(
  File "/Users/rijudey/Documents/GitHub/AppliedComputerVision/diningHallEnv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rijudey/Documents/GitHub/AppliedComputerVision/diningHallEnv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rijudey/Documents/GitHub/AppliedComputerVision/diningHallEnv/lib/python3.9/site-packages/transformers/models/owlv2/modeling_owlv2.py", line 497, in forward
    hidden_states, attn_weights = self.self_attn(
  File "/Users/rijudey/Documents/GitHub/AppliedComputerVision/diningHallEnv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/rijudey/Documents/GitHub/AppliedComputerVision/diningHallEnv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/rijudey/Documents/GitHub/AppliedComputerVision/diningHallEnv/lib/python3.9/site-packages/transformers/models/owlv2/modeling_owlv2.py", line 417, in forward
    attn_weights = nn.functional.softmax(attn_weights, dim=-1)
  File "/Users/rijudey/Documents/GitHub/AppliedComputerVision/diningHallEnv/lib/python3.9/site-packages/torch/nn/functional.py", line 1858, in softmax
    ret = input.softmax(dim)
RuntimeError: MPS backend out of memory (MPS allocated: 11.91 GB, other allocations: 661.80 MB, max allowed: 18.13 GB). Tried to allocate 5.80 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).