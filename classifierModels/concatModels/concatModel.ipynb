{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Late Fusion Concatenation Model\n",
    "Inspired by Homework 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x2963bca60>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# we want to load in a pretrained resnet model.\n",
    "# we want to use the ImageFolder format specified by PyTorch\n",
    "# we freeze the resnet parameters and train on our new dataset.\n",
    "# train and evaluate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import glob\n",
    "import itertools\n",
    "from PIL import Image\n",
    "from PIL.Image import BILINEAR\n",
    "from torchinfo import summary\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    ResNetForImageClassification,\n",
    "    Owlv2VisionModel,\n",
    "    ResNetModel,\n",
    "    AutoProcessor\n",
    ")\n",
    "import evaluate\n",
    "import accelerate\n",
    "\n",
    "\n",
    "cudnn.benchmark = True\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For straightforward datasets, sometimes you can make do with built-in PyTorch dataset objects.\n",
    "# We want to apply automated data augmentations, which will be different for the training\n",
    "# and eval scenarios\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3', '4', '5']\n",
      "dict_keys(['train', 'val'])\n",
      "label 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.3929), tensor(0.2438), torch.Size([1, 3, 224, 224]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"../../data/dataset/\"\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "print(class_names)\n",
    "\n",
    "print(image_datasets.keys())\n",
    "print(\"label\", image_datasets['train'][0][1])\n",
    "# we will use this test image to do all our preliminary testing to make sure stuff works.\n",
    "test_image = image_datasets['train'][0][0]\n",
    "test_image = test_image.unsqueeze(0)\n",
    "test_image.mean(), test_image.std(), test_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Concatenation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## OK\n",
    "## We need to just make this take in the pooled output of a OwLViT and a YOLO model, along with a resnet.\n",
    "## Early fusion model.\n",
    "#device = \"cpu\"  \n",
    "class BaseLineModel(nn.Module):\n",
    "  def __init__(self,\n",
    "               vit,\n",
    "               resnet,\n",
    "               tokenizer,\n",
    "               device):\n",
    "    super().__init__()\n",
    "\n",
    "    self.vit = vit\n",
    "    self.vit.eval()\n",
    "    self.vit.to(device)\n",
    "    \n",
    "    self.resnet = resnet\n",
    "    self.resnet.to(device)\n",
    "    self.resnet.eval()\n",
    "\n",
    "    self.tokenizer = tokenizer\n",
    "    self.device = device\n",
    "    \n",
    "    self.concatenatedLayerSize = vit.config.hidden_size + 1000\n",
    "    self.clf = nn.Linear(self.concatenatedLayerSize, 5)\n",
    "    print(self.concatenatedLayerSize)\n",
    "\n",
    "  def forward(self, images):\n",
    "      # Computing image embeddings\n",
    "      images = images.to(self.device)\n",
    "      image_embeddings = self.resnet(images).logits\n",
    "      print(\"image embeddings shape: \", image_embeddings.shape)\n",
    "      \n",
    "      # Computing caption embeddings\n",
    "      # tokenize all captions\n",
    "      inputs = self.tokenizer(images = images, return_tensors=\"pt\", do_rescale=False).to(self.device)\n",
    "      #Pass the tokenized captions through the BERT model\n",
    "      vit_output = self.vit(**inputs)\n",
    "\n",
    "      #get the pooler output from the BERT model's output\n",
    "      pooled_output = vit_output.pooler_output\n",
    "\n",
    "      # Concatenate image and caption embeddings along the batch dimension\n",
    "      full_embeddings = torch.cat((image_embeddings, pooled_output), dim=1)\n",
    "\n",
    "      print(full_embeddings.shape)\n",
    "      return self.clf(full_embeddings)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in pretrained Resnet and OwLViT Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "resnet = ResNetForImageClassification.from_pretrained(\"microsoft/resnet-50\")\n",
    "vit = Owlv2VisionModel.from_pretrained(\"google/owlv2-base-patch16\")\n",
    "processor = AutoProcessor.from_pretrained(\"google/owlv2-base-patch16\")\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = resnet(test_image)\n",
    "    print(output.logits.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(test_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1768\n",
      "image embeddings shape:  torch.Size([1, 1000])\n",
      "torch.Size([1, 1768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6318,  2.4691, -3.3295, -0.3652,  3.9658]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatModel = BaseLineModel(vit = vit, resnet = resnet, tokenizer = processor, device = \"cpu\")\n",
    "\n",
    "concatModel(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Owlv2VisionModel' object has no attribute 'hidden_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m      5\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mpooler_output  \u001b[38;5;66;03m# pooled CLS states\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mvit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/GitHub/AppliedComputerVision/diningHallEnv/lib/python3.9/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Owlv2VisionModel' object has no attribute 'hidden_size'"
     ]
    }
   ],
   "source": [
    "inputs = processor(images=test_image, return_tensors=\"pt\", do_rescale = False)\n",
    "outputs = vit(**inputs)\n",
    "\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "pooled_output = outputs.pooler_output  # pooled CLS states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diningHallEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
