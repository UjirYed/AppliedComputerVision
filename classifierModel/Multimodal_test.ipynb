{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIFo4bjQ3dDd",
        "outputId": "6410b891-5187-48cb-e5e0-427de801d97e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\normb\\.pyenv\\pyenv-win\\versions\\3.12.2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_ids torch.Size([2, 16])\n",
            "attention_mask torch.Size([2, 16])\n",
            "pixel_values torch.Size([1, 3, 960, 960])\n",
            "Detected a photo of a cat with confidence 0.614 at location [341.67, 17.54, 642.32, 278.51]\n",
            "Detected a photo of a cat with confidence 0.665 at location [6.75, 38.97, 326.62, 354.85]\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "from transformers import Owlv2Processor, Owlv2ForObjectDetection\n",
        "\n",
        "processor = Owlv2Processor.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n",
        "\n",
        "model = Owlv2ForObjectDetection.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n",
        "\n",
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "texts = [[\"a photo of a cat\", \"a photo of a dog\"]]\n",
        "\n",
        "inputs = processor(text=texts, images=image, return_tensors=\"pt\")\n",
        "\n",
        "for k,v in inputs.items():\n",
        "  print(k,v.shape)\n",
        "\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# Target image sizes (height, width) to rescale box predictions [batch_size, 2]\n",
        "\n",
        "target_sizes = torch.Tensor([image.size[::-1]])\n",
        "\n",
        "# Convert outputs (bounding boxes and class logits) to Pascal VOC Format (xmin, ymin, xmax, ymax)\n",
        "\n",
        "results = processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=0.1)\n",
        "\n",
        "i = 0  # Retrieve predictions for the first image for the corresponding text queries\n",
        "\n",
        "text = texts[i]\n",
        "\n",
        "boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n",
        "\n",
        "for box, score, label in zip(boxes, scores, labels):\n",
        "    box = [round(i, 2) for i in box.tolist()]\n",
        "    print(f\"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hKvzLKKk3nyC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer: owlv2.logit_scale | Size: torch.Size([])\n",
            "Layer: owlv2.text_model.embeddings.token_embedding.weight | Size: torch.Size([49408, 512])\n",
            "Layer: owlv2.text_model.embeddings.position_embedding.weight | Size: torch.Size([16, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.0.self_attn.k_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.0.self_attn.k_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.0.self_attn.v_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.0.self_attn.v_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.0.self_attn.q_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.0.self_attn.q_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.0.self_attn.out_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.0.self_attn.out_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.0.layer_norm1.weight | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.0.layer_norm1.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.0.mlp.fc1.weight | Size: torch.Size([2048, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.0.mlp.fc1.bias | Size: torch.Size([2048])\n",
            "Layer: owlv2.text_model.encoder.layers.0.mlp.fc2.weight | Size: torch.Size([512, 2048])\n",
            "Layer: owlv2.text_model.encoder.layers.0.mlp.fc2.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.0.layer_norm2.weight | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.0.layer_norm2.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.1.self_attn.k_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.1.self_attn.k_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.1.self_attn.v_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.1.self_attn.v_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.1.self_attn.q_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.1.self_attn.q_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.1.self_attn.out_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.1.self_attn.out_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.1.layer_norm1.weight | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.1.layer_norm1.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.1.mlp.fc1.weight | Size: torch.Size([2048, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.1.mlp.fc1.bias | Size: torch.Size([2048])\n",
            "Layer: owlv2.text_model.encoder.layers.1.mlp.fc2.weight | Size: torch.Size([512, 2048])\n",
            "Layer: owlv2.text_model.encoder.layers.1.mlp.fc2.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.1.layer_norm2.weight | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.1.layer_norm2.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.2.self_attn.k_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.2.self_attn.k_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.2.self_attn.v_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.2.self_attn.v_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.2.self_attn.q_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.2.self_attn.q_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.2.self_attn.out_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.2.self_attn.out_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.2.layer_norm1.weight | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.2.layer_norm1.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.2.mlp.fc1.weight | Size: torch.Size([2048, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.2.mlp.fc1.bias | Size: torch.Size([2048])\n",
            "Layer: owlv2.text_model.encoder.layers.2.mlp.fc2.weight | Size: torch.Size([512, 2048])\n",
            "Layer: owlv2.text_model.encoder.layers.2.mlp.fc2.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.2.layer_norm2.weight | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.2.layer_norm2.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.3.self_attn.k_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.3.self_attn.k_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.3.self_attn.v_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.3.self_attn.v_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.3.self_attn.q_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.3.self_attn.q_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.3.self_attn.out_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.3.self_attn.out_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.3.layer_norm1.weight | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.3.layer_norm1.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.3.mlp.fc1.weight | Size: torch.Size([2048, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.3.mlp.fc1.bias | Size: torch.Size([2048])\n",
            "Layer: owlv2.text_model.encoder.layers.3.mlp.fc2.weight | Size: torch.Size([512, 2048])\n",
            "Layer: owlv2.text_model.encoder.layers.3.mlp.fc2.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.3.layer_norm2.weight | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.3.layer_norm2.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.4.self_attn.k_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.4.self_attn.k_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.4.self_attn.v_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.4.self_attn.v_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.4.self_attn.q_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.4.self_attn.q_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.4.self_attn.out_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.4.self_attn.out_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.4.layer_norm1.weight | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.4.layer_norm1.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.4.mlp.fc1.weight | Size: torch.Size([2048, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.4.mlp.fc1.bias | Size: torch.Size([2048])\n",
            "Layer: owlv2.text_model.encoder.layers.4.mlp.fc2.weight | Size: torch.Size([512, 2048])\n",
            "Layer: owlv2.text_model.encoder.layers.4.mlp.fc2.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.4.layer_norm2.weight | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.4.layer_norm2.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.5.self_attn.k_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.5.self_attn.k_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.5.self_attn.v_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.5.self_attn.v_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.5.self_attn.q_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.5.self_attn.q_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.5.self_attn.out_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.5.self_attn.out_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.5.layer_norm1.weight | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.5.layer_norm1.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.5.mlp.fc1.weight | Size: torch.Size([2048, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.5.mlp.fc1.bias | Size: torch.Size([2048])\n",
            "Layer: owlv2.text_model.encoder.layers.5.mlp.fc2.weight | Size: torch.Size([512, 2048])\n",
            "Layer: owlv2.text_model.encoder.layers.5.mlp.fc2.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.5.layer_norm2.weight | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.5.layer_norm2.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.6.self_attn.k_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.6.self_attn.k_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.6.self_attn.v_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.6.self_attn.v_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.6.self_attn.q_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.6.self_attn.q_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.6.self_attn.out_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.6.self_attn.out_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.6.layer_norm1.weight | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.6.layer_norm1.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.6.mlp.fc1.weight | Size: torch.Size([2048, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.6.mlp.fc1.bias | Size: torch.Size([2048])\n",
            "Layer: owlv2.text_model.encoder.layers.6.mlp.fc2.weight | Size: torch.Size([512, 2048])\n",
            "Layer: owlv2.text_model.encoder.layers.6.mlp.fc2.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.6.layer_norm2.weight | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.6.layer_norm2.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.7.self_attn.k_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.7.self_attn.k_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.7.self_attn.v_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.7.self_attn.v_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.7.self_attn.q_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.7.self_attn.q_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.7.self_attn.out_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.7.self_attn.out_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.7.layer_norm1.weight | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.7.layer_norm1.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.7.mlp.fc1.weight | Size: torch.Size([2048, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.7.mlp.fc1.bias | Size: torch.Size([2048])\n",
            "Layer: owlv2.text_model.encoder.layers.7.mlp.fc2.weight | Size: torch.Size([512, 2048])\n",
            "Layer: owlv2.text_model.encoder.layers.7.mlp.fc2.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.7.layer_norm2.weight | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.7.layer_norm2.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.8.self_attn.k_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.8.self_attn.k_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.8.self_attn.v_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.8.self_attn.v_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.8.self_attn.q_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.8.self_attn.q_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.8.self_attn.out_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.8.self_attn.out_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.8.layer_norm1.weight | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.8.layer_norm1.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.8.mlp.fc1.weight | Size: torch.Size([2048, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.8.mlp.fc1.bias | Size: torch.Size([2048])\n",
            "Layer: owlv2.text_model.encoder.layers.8.mlp.fc2.weight | Size: torch.Size([512, 2048])\n",
            "Layer: owlv2.text_model.encoder.layers.8.mlp.fc2.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.8.layer_norm2.weight | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.8.layer_norm2.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.9.self_attn.k_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.9.self_attn.k_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.9.self_attn.v_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.9.self_attn.v_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.9.self_attn.q_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.9.self_attn.q_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.9.self_attn.out_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.9.self_attn.out_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.9.layer_norm1.weight | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.9.layer_norm1.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.9.mlp.fc1.weight | Size: torch.Size([2048, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.9.mlp.fc1.bias | Size: torch.Size([2048])\n",
            "Layer: owlv2.text_model.encoder.layers.9.mlp.fc2.weight | Size: torch.Size([512, 2048])\n",
            "Layer: owlv2.text_model.encoder.layers.9.mlp.fc2.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.9.layer_norm2.weight | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.9.layer_norm2.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.10.self_attn.k_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.10.self_attn.k_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.10.self_attn.v_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.10.self_attn.v_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.10.self_attn.q_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.10.self_attn.q_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.10.self_attn.out_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.10.self_attn.out_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.10.layer_norm1.weight | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.10.layer_norm1.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.10.mlp.fc1.weight | Size: torch.Size([2048, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.10.mlp.fc1.bias | Size: torch.Size([2048])\n",
            "Layer: owlv2.text_model.encoder.layers.10.mlp.fc2.weight | Size: torch.Size([512, 2048])\n",
            "Layer: owlv2.text_model.encoder.layers.10.mlp.fc2.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.10.layer_norm2.weight | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.10.layer_norm2.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.11.self_attn.k_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.11.self_attn.k_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.11.self_attn.v_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.11.self_attn.v_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.11.self_attn.q_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.11.self_attn.q_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.11.self_attn.out_proj.weight | Size: torch.Size([512, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.11.self_attn.out_proj.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.11.layer_norm1.weight | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.11.layer_norm1.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.11.mlp.fc1.weight | Size: torch.Size([2048, 512])\n",
            "Layer: owlv2.text_model.encoder.layers.11.mlp.fc1.bias | Size: torch.Size([2048])\n",
            "Layer: owlv2.text_model.encoder.layers.11.mlp.fc2.weight | Size: torch.Size([512, 2048])\n",
            "Layer: owlv2.text_model.encoder.layers.11.mlp.fc2.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.11.layer_norm2.weight | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.encoder.layers.11.layer_norm2.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.final_layer_norm.weight | Size: torch.Size([512])\n",
            "Layer: owlv2.text_model.final_layer_norm.bias | Size: torch.Size([512])\n",
            "Layer: owlv2.vision_model.embeddings.class_embedding | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.embeddings.patch_embedding.weight | Size: torch.Size([768, 3, 16, 16])\n",
            "Layer: owlv2.vision_model.embeddings.position_embedding.weight | Size: torch.Size([3601, 768])\n",
            "Layer: owlv2.vision_model.pre_layernorm.weight | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.pre_layernorm.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.0.self_attn.k_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.0.self_attn.k_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.0.self_attn.v_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.0.self_attn.v_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.0.self_attn.q_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.0.self_attn.q_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.0.self_attn.out_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.0.self_attn.out_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.0.layer_norm1.weight | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.0.layer_norm1.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.0.mlp.fc1.weight | Size: torch.Size([3072, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.0.mlp.fc1.bias | Size: torch.Size([3072])\n",
            "Layer: owlv2.vision_model.encoder.layers.0.mlp.fc2.weight | Size: torch.Size([768, 3072])\n",
            "Layer: owlv2.vision_model.encoder.layers.0.mlp.fc2.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.0.layer_norm2.weight | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.0.layer_norm2.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.1.self_attn.k_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.1.self_attn.k_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.1.self_attn.v_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.1.self_attn.v_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.1.self_attn.q_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.1.self_attn.q_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.1.self_attn.out_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.1.self_attn.out_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.1.layer_norm1.weight | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.1.layer_norm1.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.1.mlp.fc1.weight | Size: torch.Size([3072, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.1.mlp.fc1.bias | Size: torch.Size([3072])\n",
            "Layer: owlv2.vision_model.encoder.layers.1.mlp.fc2.weight | Size: torch.Size([768, 3072])\n",
            "Layer: owlv2.vision_model.encoder.layers.1.mlp.fc2.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.1.layer_norm2.weight | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.1.layer_norm2.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.2.self_attn.k_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.2.self_attn.k_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.2.self_attn.v_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.2.self_attn.v_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.2.self_attn.q_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.2.self_attn.q_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.2.self_attn.out_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.2.self_attn.out_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.2.layer_norm1.weight | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.2.layer_norm1.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.2.mlp.fc1.weight | Size: torch.Size([3072, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.2.mlp.fc1.bias | Size: torch.Size([3072])\n",
            "Layer: owlv2.vision_model.encoder.layers.2.mlp.fc2.weight | Size: torch.Size([768, 3072])\n",
            "Layer: owlv2.vision_model.encoder.layers.2.mlp.fc2.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.2.layer_norm2.weight | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.2.layer_norm2.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.3.self_attn.k_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.3.self_attn.k_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.3.self_attn.v_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.3.self_attn.v_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.3.self_attn.q_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.3.self_attn.q_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.3.self_attn.out_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.3.self_attn.out_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.3.layer_norm1.weight | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.3.layer_norm1.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.3.mlp.fc1.weight | Size: torch.Size([3072, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.3.mlp.fc1.bias | Size: torch.Size([3072])\n",
            "Layer: owlv2.vision_model.encoder.layers.3.mlp.fc2.weight | Size: torch.Size([768, 3072])\n",
            "Layer: owlv2.vision_model.encoder.layers.3.mlp.fc2.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.3.layer_norm2.weight | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.3.layer_norm2.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.4.self_attn.k_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.4.self_attn.k_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.4.self_attn.v_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.4.self_attn.v_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.4.self_attn.q_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.4.self_attn.q_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.4.self_attn.out_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.4.self_attn.out_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.4.layer_norm1.weight | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.4.layer_norm1.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.4.mlp.fc1.weight | Size: torch.Size([3072, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.4.mlp.fc1.bias | Size: torch.Size([3072])\n",
            "Layer: owlv2.vision_model.encoder.layers.4.mlp.fc2.weight | Size: torch.Size([768, 3072])\n",
            "Layer: owlv2.vision_model.encoder.layers.4.mlp.fc2.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.4.layer_norm2.weight | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.4.layer_norm2.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.5.self_attn.k_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.5.self_attn.k_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.5.self_attn.v_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.5.self_attn.v_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.5.self_attn.q_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.5.self_attn.q_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.5.self_attn.out_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.5.self_attn.out_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.5.layer_norm1.weight | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.5.layer_norm1.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.5.mlp.fc1.weight | Size: torch.Size([3072, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.5.mlp.fc1.bias | Size: torch.Size([3072])\n",
            "Layer: owlv2.vision_model.encoder.layers.5.mlp.fc2.weight | Size: torch.Size([768, 3072])\n",
            "Layer: owlv2.vision_model.encoder.layers.5.mlp.fc2.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.5.layer_norm2.weight | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.5.layer_norm2.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.6.self_attn.k_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.6.self_attn.k_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.6.self_attn.v_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.6.self_attn.v_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.6.self_attn.q_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.6.self_attn.q_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.6.self_attn.out_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.6.self_attn.out_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.6.layer_norm1.weight | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.6.layer_norm1.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.6.mlp.fc1.weight | Size: torch.Size([3072, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.6.mlp.fc1.bias | Size: torch.Size([3072])\n",
            "Layer: owlv2.vision_model.encoder.layers.6.mlp.fc2.weight | Size: torch.Size([768, 3072])\n",
            "Layer: owlv2.vision_model.encoder.layers.6.mlp.fc2.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.6.layer_norm2.weight | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.6.layer_norm2.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.7.self_attn.k_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.7.self_attn.k_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.7.self_attn.v_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.7.self_attn.v_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.7.self_attn.q_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.7.self_attn.q_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.7.self_attn.out_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.7.self_attn.out_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.7.layer_norm1.weight | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.7.layer_norm1.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.7.mlp.fc1.weight | Size: torch.Size([3072, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.7.mlp.fc1.bias | Size: torch.Size([3072])\n",
            "Layer: owlv2.vision_model.encoder.layers.7.mlp.fc2.weight | Size: torch.Size([768, 3072])\n",
            "Layer: owlv2.vision_model.encoder.layers.7.mlp.fc2.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.7.layer_norm2.weight | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.7.layer_norm2.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.8.self_attn.k_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.8.self_attn.k_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.8.self_attn.v_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.8.self_attn.v_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.8.self_attn.q_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.8.self_attn.q_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.8.self_attn.out_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.8.self_attn.out_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.8.layer_norm1.weight | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.8.layer_norm1.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.8.mlp.fc1.weight | Size: torch.Size([3072, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.8.mlp.fc1.bias | Size: torch.Size([3072])\n",
            "Layer: owlv2.vision_model.encoder.layers.8.mlp.fc2.weight | Size: torch.Size([768, 3072])\n",
            "Layer: owlv2.vision_model.encoder.layers.8.mlp.fc2.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.8.layer_norm2.weight | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.8.layer_norm2.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.9.self_attn.k_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.9.self_attn.k_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.9.self_attn.v_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.9.self_attn.v_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.9.self_attn.q_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.9.self_attn.q_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.9.self_attn.out_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.9.self_attn.out_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.9.layer_norm1.weight | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.9.layer_norm1.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.9.mlp.fc1.weight | Size: torch.Size([3072, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.9.mlp.fc1.bias | Size: torch.Size([3072])\n",
            "Layer: owlv2.vision_model.encoder.layers.9.mlp.fc2.weight | Size: torch.Size([768, 3072])\n",
            "Layer: owlv2.vision_model.encoder.layers.9.mlp.fc2.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.9.layer_norm2.weight | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.9.layer_norm2.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.10.self_attn.k_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.10.self_attn.k_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.10.self_attn.v_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.10.self_attn.v_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.10.self_attn.q_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.10.self_attn.q_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.10.self_attn.out_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.10.self_attn.out_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.10.layer_norm1.weight | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.10.layer_norm1.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.10.mlp.fc1.weight | Size: torch.Size([3072, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.10.mlp.fc1.bias | Size: torch.Size([3072])\n",
            "Layer: owlv2.vision_model.encoder.layers.10.mlp.fc2.weight | Size: torch.Size([768, 3072])\n",
            "Layer: owlv2.vision_model.encoder.layers.10.mlp.fc2.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.10.layer_norm2.weight | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.10.layer_norm2.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.11.self_attn.k_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.11.self_attn.k_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.11.self_attn.v_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.11.self_attn.v_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.11.self_attn.q_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.11.self_attn.q_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.11.self_attn.out_proj.weight | Size: torch.Size([768, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.11.self_attn.out_proj.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.11.layer_norm1.weight | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.11.layer_norm1.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.11.mlp.fc1.weight | Size: torch.Size([3072, 768])\n",
            "Layer: owlv2.vision_model.encoder.layers.11.mlp.fc1.bias | Size: torch.Size([3072])\n",
            "Layer: owlv2.vision_model.encoder.layers.11.mlp.fc2.weight | Size: torch.Size([768, 3072])\n",
            "Layer: owlv2.vision_model.encoder.layers.11.mlp.fc2.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.11.layer_norm2.weight | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.encoder.layers.11.layer_norm2.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.post_layernorm.weight | Size: torch.Size([768])\n",
            "Layer: owlv2.vision_model.post_layernorm.bias | Size: torch.Size([768])\n",
            "Layer: owlv2.visual_projection.weight | Size: torch.Size([512, 768])\n",
            "Layer: owlv2.text_projection.weight | Size: torch.Size([512, 512])\n",
            "Layer: class_head.dense0.weight | Size: torch.Size([512, 768])\n",
            "Layer: class_head.dense0.bias | Size: torch.Size([512])\n",
            "Layer: class_head.logit_shift.weight | Size: torch.Size([1, 768])\n",
            "Layer: class_head.logit_shift.bias | Size: torch.Size([1])\n",
            "Layer: class_head.logit_scale.weight | Size: torch.Size([1, 768])\n",
            "Layer: class_head.logit_scale.bias | Size: torch.Size([1])\n",
            "Layer: box_head.dense0.weight | Size: torch.Size([768, 768])\n",
            "Layer: box_head.dense0.bias | Size: torch.Size([768])\n",
            "Layer: box_head.dense1.weight | Size: torch.Size([768, 768])\n",
            "Layer: box_head.dense1.bias | Size: torch.Size([768])\n",
            "Layer: box_head.dense2.weight | Size: torch.Size([4, 768])\n",
            "Layer: box_head.dense2.bias | Size: torch.Size([4])\n",
            "Layer: objectness_head.dense0.weight | Size: torch.Size([768, 768])\n",
            "Layer: objectness_head.dense0.bias | Size: torch.Size([768])\n",
            "Layer: objectness_head.dense1.weight | Size: torch.Size([768, 768])\n",
            "Layer: objectness_head.dense1.bias | Size: torch.Size([768])\n",
            "Layer: objectness_head.dense2.weight | Size: torch.Size([1, 768])\n",
            "Layer: objectness_head.dense2.bias | Size: torch.Size([1])\n",
            "Layer: layer_norm.weight | Size: torch.Size([768])\n",
            "Layer: layer_norm.bias | Size: torch.Size([768])\n"
          ]
        }
      ],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    print(f\"Layer: {name} | Size: {param.size()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BcvQz9O65AB",
        "outputId": "fd0b2989-835f-4025-866e-ad872bf91e28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "troT4jOM4erR",
        "outputId": "2fec35fd-2cdd-43c3-f057-118641d2e04b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "===================================================================================================================\n",
              "Layer (type:depth-idx)                                            Output Shape              Param #\n",
              "===================================================================================================================\n",
              "Owlv2ForObjectDetection                                           [1, 3601, 768]            --\n",
              "├─Owlv2Model: 1-1                                                 [1, 3601, 768]            1\n",
              "│    └─Owlv2VisionTransformer: 2-1                                [1, 768]                  --\n",
              "│    │    └─Owlv2VisionEmbeddings: 3-1                            [1, 3601, 768]            3,356,160\n",
              "│    │    └─LayerNorm: 3-2                                        [1, 3601, 768]            1,536\n",
              "│    │    └─Owlv2Encoder: 3-3                                     [1, 3601, 768]            85,054,464\n",
              "│    │    └─LayerNorm: 3-4                                        [1, 768]                  1,536\n",
              "│    └─Owlv2TextTransformer: 2-2                                  [2, 512]                  --\n",
              "│    │    └─Owlv2TextEmbeddings: 3-5                              [2, 16, 512]              25,305,088\n",
              "│    │    └─Owlv2Encoder: 3-6                                     [2, 16, 512]              37,828,608\n",
              "│    │    └─LayerNorm: 3-7                                        [2, 16, 512]              1,024\n",
              "│    └─Linear: 2-3                                                [2, 512]                  262,144\n",
              "│    └─Linear: 2-4                                                [1, 512]                  393,216\n",
              "│    └─Owlv2VisionTransformer: 2-5                                --                        (recursive)\n",
              "│    │    └─LayerNorm: 3-8                                        [1, 3601, 768]            (recursive)\n",
              "├─LayerNorm: 1-2                                                  [1, 3600, 768]            1,536\n",
              "├─Owlv2ClassPredictionHead: 1-3                                   [1, 3600, 2]              --\n",
              "│    └─Linear: 2-6                                                [1, 3600, 512]            393,728\n",
              "│    └─Linear: 2-7                                                [1, 3600, 1]              769\n",
              "│    └─Linear: 2-8                                                [1, 3600, 1]              769\n",
              "│    └─ELU: 2-9                                                   [1, 3600, 1]              --\n",
              "├─Owlv2BoxPredictionHead: 1-4                                     [1, 3600, 1]              --\n",
              "│    └─Linear: 2-10                                               [1, 3600, 768]            590,592\n",
              "│    └─GELU: 2-11                                                 [1, 3600, 768]            --\n",
              "│    └─Linear: 2-12                                               [1, 3600, 768]            590,592\n",
              "│    └─GELU: 2-13                                                 [1, 3600, 768]            --\n",
              "│    └─Linear: 2-14                                               [1, 3600, 1]              769\n",
              "├─Owlv2BoxPredictionHead: 1-5                                     [1, 3600, 4]              --\n",
              "│    └─Linear: 2-15                                               [1, 3600, 768]            590,592\n",
              "│    └─GELU: 2-16                                                 [1, 3600, 768]            --\n",
              "│    └─Linear: 2-17                                               [1, 3600, 768]            590,592\n",
              "│    └─GELU: 2-18                                                 [1, 3600, 768]            --\n",
              "│    └─Linear: 2-19                                               [1, 3600, 4]              3,076\n",
              "├─Sigmoid: 1-6                                                    [1, 3600, 4]              --\n",
              "===================================================================================================================\n",
              "Total params: 154,966,792\n",
              "Trainable params: 154,966,792\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 2.34\n",
              "===================================================================================================================\n",
              "Input size (MB): 11.06\n",
              "Forward/backward pass size (MB): 3152.12\n",
              "Params size (MB): 619.86\n",
              "Estimated Total Size (MB): 3783.04\n",
              "==================================================================================================================="
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "\n",
        "print(type(inputs))\n",
        "\n",
        "inputsDict = {k: v for k,v in inputs.items()}\n",
        "# Print model summary\n",
        "summary(model, input_data=inputsDict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDKmYukA5ZgJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
